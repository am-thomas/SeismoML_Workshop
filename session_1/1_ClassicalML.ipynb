{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEIOb8ylZSau"
      },
      "source": [
        "**ML Exercise 1: Train and evaluate a Logistic Regression model**\n",
        "\n",
        "In this short exercise, we will build a classical machine learning model (Logistic Regression) using a few statistical features based on the STA/LTA ratio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVU2VzxzdF-w"
      },
      "source": [
        "1. Download a small dataset of earthquake and noise samples [here](https://github.com/am-thomas/SeismoML_Workshop/blob/main/session_1/CML_dataset.csv). The dataset contains six STA/LTA-based features corresponding to 1000 earthquakes ('E') and 1000 noise ('N') samples. Upload the dataset csv file to Google Colab by selecting the folder icon on the righthand menu.\n",
        "\n",
        "2. Read and visualize the dataset by running the cells below.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSKyp-yrPtME"
      },
      "outputs": [],
      "source": [
        "# import required packages\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection, metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Se9HJea2Gct_",
        "outputId": "38de2ef9-5b88-490a-fbc1-95a7d044c9af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  network_code receiver_code               stw_start_time  stalta_Z  \\\n",
            "0           PB          B086  2007-12-20T09:25:38.760000Z  5.188151   \n",
            "1           NN           WCN  2008-06-18T11:15:28.041760Z  1.260020   \n",
            "2           AZ           BZN  2013-09-19T07:13:05.730000Z  1.518906   \n",
            "3           HV          AHUD  2018-07-14T13:26:09.510000Z  6.697070   \n",
            "4           PB          B067  2014-10-23T03:08:41.220000Z  5.693423   \n",
            "\n",
            "          sta_Z  stalta_N         sta_N  stalta_E         sta_E label  \n",
            "0  1.320000e-08  5.683296  1.320000e-08  4.525951  1.160000e-08     E  \n",
            "1  8.170000e-08  1.129663  1.050000e-07  1.009612  4.860000e-08     E  \n",
            "2  2.330000e-07  1.821325  2.910000e-07  1.453542  3.250000e-07     E  \n",
            "3  1.610000e-06  4.710274  5.780000e-06  4.946807  4.450000e-06     E  \n",
            "4  3.330000e-08  5.835384  2.970000e-08  9.409992  3.670000e-08     E  \n"
          ]
        }
      ],
      "source": [
        "# read in the dataset and print first five lines with headers\n",
        "dataset_df = pd.read_csv('CML_dataset.csv')\n",
        "print(dataset_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ew_p3odlbPnT"
      },
      "source": [
        "\n",
        "3. Select two features and create a feature array *X* with shape (2000, 2) for two features of 2000 data samples. Store a label array *y*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRLUA_wcGw9k"
      },
      "outputs": [],
      "source": [
        "# fill in the appropriate column names to create a feature (X) and a label (y) array\n",
        "X = dataset_df[['#FEATURE1#', '#FEATURE2#']]\n",
        "y = dataset_df['#INSERT#']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SdfJjaaZ_cC"
      },
      "source": [
        "2. Read throught the documentation of the sklearn.model_selection function [train_test_split() ](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). Create a split of the dataset where 80% of the dataset is reserved for training and 20% is for testing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GDXO-zIZ56U"
      },
      "outputs": [],
      "source": [
        "# fill in the appropriate parameters to create a 80/20 train-test split\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split('#INSERT#')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ6I1rEcc5K0"
      },
      "source": [
        "3. Train your model and apply it to your testing set. Look through the methods of  [LogisticRegression()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression) to fill in the appropriate arguments below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyJhNgerLXea"
      },
      "outputs": [],
      "source": [
        "# create an instance of a Logistic Regression model\n",
        "LG_model = LogisticRegression()\n",
        "\n",
        "# train/fit your model with the training set\n",
        "LG_model.fit('#INSERT#')\n",
        "\n",
        "# apply your trained model to the testing set\n",
        "predict_y_test = LG_model.predict('#INSERT#')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Df1aq_ZiPfh"
      },
      "source": [
        "3. Read through the documentation of the sklearn.metrics function [confusion_matrix()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) function. Let us consider the Earthquake/'E' class to be the positive class. Store the true negative true positives (TP), false negatives (FN), false positives (FP), and true negatives (TN).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyCzzp-Edgu0"
      },
      "outputs": [],
      "source": [
        "TP, FN, FP, TN = metrics.confusion_matrix('#INSERT#').ravel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hTvnekhkURb"
      },
      "source": [
        "**Model Evaluation**\n",
        "\n",
        "Four common metrics for evaluating the performance of an ML model are accuracy, recall, precision, and f1-score. Accuracy measures the proportion of total samples that the model predicts correctly. It can be computed as follows:\n",
        "    \n",
        "$$Accuracy = \\frac{TP + TN}{TP +TN + FP + FN}$$\n",
        "  \n",
        "  \n",
        "When the cost of missing a postive (false negative) is high (usually the case for earthquake detection), recall may be a preferred metric. Recall or sensitivity measures the proportion of all positive samples that were correctly classified by the model:\n",
        "$$ Recall = \\frac{TP}{TP + FN}$$\n",
        "\n",
        "Precision can be an useful metric when the cost of false positives is heigh. Precision measures the proportion of all predicted postive samples that are true positives:\n",
        "$$ Precision = \\frac{TP}{TP + FP}$$   \n",
        "\n",
        "The F1 score is the harmonic mean of recall and precision. It takes into account the cost of both false positives and false negatives.\n",
        "$$ F1 \\ Score = \\frac{2 \\times Recall \\times Precision}{ Recall + Precision} $$\n",
        "\n",
        "As we will discuss later, we can combine these basal metrics with dataset parameters to provide a more robust evaluation of a ML model.\n",
        "\n",
        "4. Use the equations above to compute the accuracy, recall, precision, and f1 score of your model.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63CnEvZ0Vj2n"
      },
      "outputs": [],
      "source": [
        "acc =\n",
        "rec =\n",
        "prec =\n",
        "f1 ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJOJbJ-UnYMB"
      },
      "source": [
        "5. Confirm that your methodology was correct by comparing your values with scikit-learn's values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxJGUH5qk3WU",
        "outputId": "17b02fe3-e238-4648-fb45-2017cab23454"
      },
      "outputs": [],
      "source": [
        "# print the four performance metrics when the postive class (pos_label) is 'E'\n",
        "print('Accuracy:', metrics.accuracy_score(y_test, predict_y_test))\n",
        "print('Precison:', metrics.precision_score(y_test, predict_y_test, pos_label='E'))\n",
        "print('Recall:', metrics.recall_score(y_test, predict_y_test, pos_label='E'))\n",
        "print('F1-score:', metrics.f1_score(y_test, predict_y_test, pos_label='E'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDLZ_yo4nqm-"
      },
      "source": [
        "6. Create an X array with all six features in the dataset and redo the previous steps. Compute how much your accuracy, precision, recall, and F1-score changes with the new model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yM7uuNKKo1Nu"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "\n",
        "# store the differences in performance between a 2- and 6-feature model\n",
        "diff_acc =\n",
        "diff_rec =\n",
        "diff_prec =\n",
        "diff_f1 ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOy6xGxhpa5F"
      },
      "source": [
        "There are numerous ways in which we can improve our earthquake detection model even further. Let us brainstorm together. Add your ideas for model improvement to this [JamBoard](https://jamboard.google.com/d/1p6NJoTeADR6eznGCww44sAVl9xUk4Ofh7zNu5r1PxJ4/edit?usp=sharing)!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
